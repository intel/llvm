= SYCL_INTEL_extended_atomics

:source-highlighter: coderay
:coderay-linenums-mode: table

// This section needs to be after the document title.
:doctype: book
:toc2:
:toc: left
:encoding: utf-8
:lang: en

:blank: pass:[ +]

// Set the default source code type in this document to C++,
// for syntax highlighting purposes.  This is needed because
// docbook uses c++ and html5 uses cpp.
:language: {basebackend@docbook:c++:cpp}

// This is necessary for asciidoc, but not for asciidoctor
:cpp: C++

== Introduction
IMPORTANT: This specification is a draft.

NOTE: Khronos(R) is a registered trademark and SYCL(TM) and SPIR(TM) are trademarks of The Khronos Group Inc.  OpenCL(TM) is a trademark of Apple Inc. used by permission by Khronos.

NOTE: This document is better viewed when rendered as html with asciidoctor.  GitHub does not render image icons.

This document describes an extension that introduces the `cl::sycl::intel::atomic_ref` class, which exposes additional functionality aligned with the +std::atomic_ref+ class from {cpp}20.

== Name Strings

+SYCL_INTEL_extended_atomics+

== Notice

Copyright (c) 2020 Intel Corporation.  All rights reserved.

== Status

Working Draft

This is a preview extension specification, intended to provide early access to a feature for review and community feedback. When the feature matures, this specification may be released as a formal extension.

Because the interfaces defined by this specification are not final and are subject to change they are not intended to be used by shipping software products.

== Version

Built On: {docdate} +
Revision: 1

== Contact
John Pennycook, Intel (john 'dot' pennycook 'at' intel 'dot' com)

== Dependencies

This extension is written against the SYCL 1.2.1 specification, Revision 6 and the following extensions:

- Unified Shared Memory
- SYCL_INTEL_sub_groups

== Overview

The SYCL atomic library (+cl::sycl::atomic+) defined in SYCL 1.2.1 is based on the standard atomic libary (+std::atomic+) but has some differences.  This extension introduces a new fence function (+cl::sycl::intel::atomic_fence+) and an alternative atomic class (+cl::sycl::intel::atomic_ref+) including additional features from {cpp}20:

- Overloaded operators to reduce the verbosity of using atomics
- Missing functions (e.g. `is_lock_free()`)
- Support for floating-point types
- Support for additional memory orderings besides `relaxed`
- Support for scopes denoting the set of work-items and devices to which memory ordering applies

This extension deprecates the SYCL 1.2.1 +cl::sycl::atomic+ class and accessors created with mode +access::mode::atomic+.

The extension can be enabled using the `-fsycl-extended-atomics` flag, and applications can check whether the extension is enabled using `__has_extension(sycl_extended_atomics)`.

=== Overloaded Operators

In SYCL 1.2.1, the +cl::sycl::atomic+ class provides atomic operations by way of member functions (e.g. +fetch_add+) without defining the corresponding operators (e.g. `+=`).  This increases the verbosity of simple uses of atomics.

The operators defined by this extension match those defined for +std::atomic_ref+ in {cpp}20.  The functionality of each operator is equivalent to calling a corresponding member function of the +atomic_ref+ class -- the operators do not expose any new functionality of the class, but act as shorthands for common use-cases.

==== Operators for All Supported Types

[source,c++]
----
operator T();            // equivalent to load()
T operator=(T desired);  // equivalent to store(desired)
T operator+=(T operand); // equivalent to fetch_add(operand)
T operator-=(T operand); // equivalent to fetch_sub(operand)
----

==== Operators for Integral Types

[source,c++]
----
T operator++(int);         // equivalent to fetch_add(1)
T operator--(int);         // equivalent to fetch_sub(1)
T operator++();            // equivalent to fetch_add(1) + 1
T operator--();            // equivalent to fetch_sub(1) - 1
T operator&=(T operand);   // equivalent to fetch_and(operand)
T operator|=(T operand);   // equivalent to fetch_or(operand)
T operator^=(T operand);   // equivalent to fetch_xor(operand)
----

==== Operators for Pointer Types

[source,c++]
----
T operator++(int); // equivalent to fetch_add(1)
T operator--(int); // equivalent to fetch_sub(1)
T operator++();    // equivalent to fetch_add(1) + 1
T operator--();    // equivalent to fetch_sub(1) - 1
----

=== Support for Floating-point Types

In SYCL 1.2.1, support for floating-point types is limited to the +load+, +store+ and +exchange+ member functions.  Many applications requiring additional atomic operations (e.g. addition) currently work around this restriction using type punning and integer +compare_exchange+ operations.

This extension extends support for floating-point types to the +compare_exchange+, +fetch_add+ and +fetch_sub+ functions in line with {cpp}20, as well as the +fetch_min+ and +fetch_max+ functions.  These new functions do not require dedicated floating-point atomic instructions and can be emulated using integer operations, giving compilers the freedom to choose the best implementation for the target device.

=== Support for Additional Memory Orderings

The atomic operations in SYCL 1.2.1 default to +memory_order_relaxed+, which is inconsistent with the default of +memory_order_seq_cst+ used by the +std::atomic+ class.  Defaulting to +memory_order_relaxed+ may improve the performance and portability of SYCL 1.2.1 code across multiple target devices, but may also lead to unexpected behavior when code is migrated between {cpp} and SYCL.  Different users have different understandings of which memory orders are the most common or useful, and the performance difference between memory orders is also expected to vary between devices.  This extension therefore makes the default memory order of +cl::sycl::intel::atomic_ref+ dependent upon a template argument that must be specified by the user.

All devices must support +memory_order_relaxed+, and the host device must support all {cpp} memory orders.  These changes bring the SYCL memory model in line with modern {cpp} while allowing a device/compiler to implement only a subset of {cpp} memory orders.  Supporting the standard {cpp} memory model in SYCL requires that disjoint address spaces (e.g. local and global memory) are treated as though they are part of a single address space (i.e. there must be a single happens-before relationship for all addresses).

=== Support for Memory Scopes

The fact that atomic operations in SYCL 1.2.1 obey separate happens-before relationships for global and local memory enables implementations to improve performance by limiting the scope of fences and other visibility operations (e.g. cache flushes) based on the address space to which an atomic operation is applied.  The single happens-before relationship adopted by this proposal for consistency with {cpp} prevents this optimization, which may degrade performance on some architectures in some specific cases.

To address this, we introduce an additional concept of memory scope to SYCL atomics, denoting the set of work-items and devices to which the memory ordering constraint of an atomic operation must be applied.  These scopes are defined by a new enumeration class:

- +memory_scope::work_item+  
  The ordering constraint applies only to the calling work-item.  
  This is only useful for image operations, as all other operations within a work-item are guaranteed to execute in program order.

- +memory_scope::sub_group+  
  The ordering constraint applies only to work-items in the same sub-group as the calling work-item.

- +memory_scope::work_group+  
  The ordering constraint applies only to work-items in the same work-group as the calling work-item.  
  This is the broadest scope that can be applied to atomic operations in work-group local memory.  Using any broader scope for atomic operations in work-group local memory is treated as though +memory_scope::work_group+ was specified.

- +memory_scope::device+  
  The ordering constraint applies only to work-items executing on the same device as the calling work-item.

- +memory_scope::system+  
  The ordering constraint applies to any device work-item or host thread in the system that is currently permitted to access the memory allocation containing the referenced object, as defined by the capabilities of buffers and USM.  
  This scope is equivalent to +memory_scope::device+ if a device does not support Concurrent or System USM.

All devices must support +memory_scope::work_group+, and the host device must support all memory scopes.

=== The +atomic_ref+ Class

The +cl::sycl::intel::atomic_ref+ class is constructed from a reference, and enables atomic operations to the referenced object.  If any non-atomic access to the referenced object is made during the lifetime of the +cl::sycl::intel::atomic_ref+ class then the behavior is undefined.  No subobject of the object referenced by an +atomic_ref+ shall be concurrently referenced by any other +atomic_ref+ object.

The address space specified by the template argument +Space+ must be +access::address_space::global_space+ or +access::address_space::local_space+.  It is illegal for an +atomic_ref+ to reference an object in +access::address_space::constant_space+ or +access::address_space::private_space+.

The static member +required_alignment+ describes the minimum required alignment in bytes of an object that can be referenced by an +atomic_ref<T>+, which must be at least +alignof(T)+.

The static member +is_always_lock_free+ is true if all atomic operations for type +T+ are always lock-free.  A SYCL implementation is not guaranteed to support atomic operations that are not lock-free.

The static members +default_read_order+, +default_write_order+ and +default_read_modify_write_order+ reflect the default memory order values for each type of atomic operation, consistent with the +DefaultOrder+ template argument.

The static member +default_scope+ reflects the +DefaultScope+ template argument.

The member functions below are common to atomic references for any type +T+:

|===
|Member Functions|Description

| `atomic_ref(T& ref)`
| Constructs an instance of +atomic_ref+ which is associated with the object referenced by _ref_.

| `atomic_ref(const atomic_ref& ref) noexcept`
| Constructs an instance of +atomic_ref+ which is associated with the same object as _ref_.

| `bool is_lock_free() const noexcept`
| Return +true+ if the atomic operations provided by this +atomic_ref+ are lock-free.

| `void store(T operand, memory_order order = default_write_order, memory_scope scope = default_scope) const noexcept`
| Atomically stores _operand_ to the object referenced by this +atomic_ref+.  The memory order of this atomic operation must be +memory_order::relaxed+, +memory_order::release+ or +memory_order::seq_cst+.

| `T operator=(T desired) const noexcept`
| Equivalent to +store(desired)+.  Returns _desired_.

| `T load(memory_order order = default_read_order, memory_scope scope = default_scope) const noexcept`
| Atomically loads the value of the object referenced by this +atomic_ref+.  The memory order of this atomic operation must be +memory_order::relaxed+, +memory_order::acquire+, or +memory_order::seq_cst+.

| `operator T() const noexcept`
| Equivalent to +load()+.

| `T exchange(T operand, memory_order order = default_read_modify_write_order, memory_scope scope = default_scope) const noexcept`
| Atomically replaces the value of the object referenced by this +atomic_ref+ with _operand_ and returns the original value of the referenced object.

| `bool compare_exchange_weak(T &expected, T desired, memory_order success, memory_order failure, memory_scope scope = default_scope) const noexcept`
| Atomically compares the value of the object referenced by this +atomic_ref+ against the value of _expected_. If the values are equal attempts to replace the value of the referenced object with the value of +desired+, otherwise assigns the original value of the referenced object to _expected_. Returns +true+ if the comparison operation and replacement operation were successful. The _failure_ memory order of this atomic operation must be +memory_order::relaxed+, +memory_order::acquire+ or +memory_order::seq_cst+.

| `bool compare_exchange_weak(T &expected, T desired, memory_order order = default_read_modify_write_order, memory_scope scope = default_scope) const noexcept`
| Equivalent to +compare_exchange_weak(expected, desired, order, order, scope)+.

| `bool compare_exchange_strong(T &expected, T desired, memory_order success, memory_order failure, memory_scope scope = default_scope) const noexcept`
| Atomically compares the value of the object referenced by this +atomic_ref+ against the value of _expected_. If the values are equal replaces the value of the referenced object with the value of +desired+, otherwise assigns the original value of the referenced object to _expected_. Returns +true+ if the comparison operation was successful. The _failure_ memory order of this atomic operation must be +memory_order::relaxed+, +memory_order::acquire+ or +memory_order::seq_cst+.

| `bool compare_exchange_strong(T &expected, T desired, memory_order order = default_read_modify_write_order, memory_scope scope = default_scope) const noexcept`
| Equivalent to +compare_exchange_strong(expected, desired, order, order, scope)+.

|===

The additional member functions below are available for atomic references to integral types:

|===
|Member Functions|Description

| `T fetch_add(T operand, memory_order order = default_read_modify_write_order, memory_scope scope = default_scope) const noexcept`
| Atomically adds _operand_ to the value of the object referenced by this +atomic_ref+ and assigns the result to the value of the referenced object.  Returns the original value of the referenced object.

| `T operator+=(T operand) const noexcept`
| Equivalent to +fetch_add(operand)+.

| `T operator++(int) const noexcept`
| Equivalent to +fetch_add(1)+.

| `T operator++() const noexcept`
| Equivalent to +fetch_add(1) + 1+.

| `T fetch_sub(T operand, memory_order order = default_read_modify_write_order, memory_scope scope = default_scope) const noexcept`
| Atomically subtracts _operand_ from the value of the object referenced by this +atomic_ref+ and assigns the result to the value of the referenced object.  Returns the original  value of the referenced object.

| `T operator-=(T operand) const noexcept`
| Equivalent to +fetch_sub(operand)+.

| `T operator--(int) const noexcept`
| Equivalent to +fetch_sub(1)+.

| `T operator--() const noexcept`
| Equivalent to +fetch_sub(1) - 1+.

| `T fetch_and(T operand, memory_order order = default_read_modify_write_order, memory_scope scope = default_scope) const noexcept`
| Atomically performs a bitwise AND between _operand_ and the value of the object referenced by this +atomic_ref+, and assigns the result to the value of the referenced object. Returns the original value of the referenced object.

| `T operator&=(T operand) const noexcept`
| Equivalent to +fetch_and(operand)+.

| `T fetch_or(T operand, memory_order order = default_read_modify_write_order, memory_scope scope = default_scope) const noexcept`
| Atomically performs a bitwise OR between _operand_ and the value of the object referenced by this +atomic_ref+, and assigns the result to the value of the referenced object. Returns the original value of the referenced object.

| `T operator\|=(T operand) const noexcept`
| Equivalent to +fetch_or(operand)+.

| `T fetch_xor(T operand, memory_order order = default_read_modify_write_order, memory_scope scope = default_scope) const noexcept`
| Atomically performs a bitwise XOR between the value +operand+ and the value of the object referenced by this +atomic_ref+, and assigns the result to the value of the referenced object. Returns the original value of the referenced object.

| `T operator^=(T operand) const noexcept`
| Equivalent to +fetch_xor(operand)+.

| `T fetch_min(T operand, memory_order order = default_read_modify_write_order, memory_scope scope = default_scope) const noexcept`
| Atomically computes the minimum of _operand_ and the value of the object referenced by this +atomic_ref+, and assigns the result to the value of the referenced object. Returns the original value of the referenced object.

| `T fetch_max(T operand, memory_order order = default_read_modify_write_order, memory_scope scope = default_scope) const noexcept`
| Atomically computes the maximum of _operand_ and the value of the object referenced by this +atomic_ref+, and assigns the result to the value of the referenced object. Returns the original value of the referenced object.

|===

The additional member functions below are available for atomic references to floating-point types:

|===
| Member Function | Description

| `T fetch_add(T operand, memory_order order = default_read_modify_write_order, memory_scope scope = default_scope) const noexcept`
| Atomically adds _operand_ to the value of the object referenced by this +atomic_ref+ and assigns the result to the value of the referenced object.  Returns the original value of the referenced object.

| `T operator+=(T operand) const noexcept`
| Equivalent to +fetch_add(operand)+.

| `T fetch_sub(T operand, memory_order order = default_read_modify_write_order, memory_scope scope = default_scope) const noexcept`
| Atomically subtracts _operand_ from the value of the object referenced by this +atomic_ref+ and assigns the result to the value of the referenced object.  Returns the original  value of the referenced object.

| `T operator-=(T operand) const noexcept`
| Equivalent to +fetch_sub(operand)+.

| `T fetch_min(T operand, memory_order order = default_read_modify_write_order, memory_scope scope = default_scope) const noexcept`
| Atomically computes the minimum of _operand_ and the value of the object referenced by this +atomic_ref+, and assigns the result to the value of the referenced object. Returns the original value of the referenced object.

| `T fetch_max(T operand, memory_order order = default_read_modify_write_order, memory_scope scope = default_scope) const noexcept`
| Atomically computes the maximum of _operand_ and the value of the object referenced by this +atomic_ref+, and assigns the result to the value of the referenced object. Returns the original value of the referenced object.

|===

The additional member functions below are available for atomic references to pointer types:

|===
| Member Function | Description

| `T* fetch_add(difference_type operand, memory_order order = default_read_modify_write_order, memory_scope scope = default_scope) const noexcept`
| Atomically adds _operand_ to the value of the object referenced by this +atomic_ref+ and assigns the result to the value of the referenced object.  Returns the original value of the referenced object.

| `T* operator+=(difference_type operand) const noexcept`
| Equivalent to +fetch_add(operand)+.

| `T* operator++(int) const noexcept`
| Equivalent to +fetch_add(1)+.

| `T* operator++() const noexcept`
| Equivalent to +fetch_add(1) + 1+.

| `T* fetch_sub(difference_type operand, memory_order order = default_read_modify_write_order, memory_scope scope = default_scope) const noexcept`
| Atomically subtracts _operand_ from the value of the object referenced by this +atomic_ref+ and assigns the result to the value of the referenced object.  Returns the original  value of the referenced object.

| `T* operator-=(difference_type operand) const noexcept`
| Equivalent to +fetch_sub(operand)+.

| `T* operator--(int) const noexcept`
| Equivalent to +fetch_sub(1)+.

| `T* operator--() const noexcept`
| Equivalent to +fetch_sub(1) - 1+.

|===

==== Atomic Fence

The +atomic_fence+ function corresponds to the +std::atomic_thread_fence+ function, and performs a memory fence ordering accesses to any memory space across the specified +memory_scope+.

The effects of a call to +atomic_fence+ depend on the value of the +order+ parameter:

- `relaxed`: No effect
- `acquire`: Acquire fence
- `release`: Release fence
- `acq_rel`: Both an acquire fence and a release fence
- `seq_cst`: A sequentially consistent acquire and release fence

==== Atomic Accessor

The +atomic_accessor+ class replaces accessors created with +access::mode::atomic+.  All operators of +atomic_accessor+ which provide access to an element of the underlying memory do so by wrapping the element in an +atomic_ref+.  In order to guarantee that all accesses to the underlying memory are atomic, an +atomic_accessor+ does not allow direct access to the memory via +get_pointer()+.

All other member functions are as defined in the +accessor+ class.

|===
| Member Function | Description

| `operator atomic_ref<DataT, DefaultOrder, DefaultScope, access::address_space::global_space>() const`
| Available only when: `Dimensions == 0`.  Returns an `atomic_ref` associated with the element stored in the underlying `buffer` or work-group local memory.

| `atomic_ref<DataT, DefaultOrder, DefaultScope, access::address_space::global_space> operator[](id<Dimensions> index) const`
| Available only when: `Dimensions > 0`.  Returns an `atomic_ref` associated with the element stored at the specified _index_ in the underlying `buffer` or work-group local memory.

| `atomic_ref<DataT, DefaultOrder, DefaultScope, access::address_space::global_space> operator[](size_t index) const`
| Available only when: `Dimensions == 1`.  Returns an `atomic_ref` associated with the element stored at the specified _index_ in the underlying `buffer` or work-group local memory.

| `global_ptr<DataT> get_pointer() const = delete`
| Direct access to the underlying `buffer` or work-group local memory is not permitted.

|===

To simplify the construction of an +atomic_accessor+, tag objects of type +order_tag_t+ and +scope_tag_t+ may optionally be passed to the constructor.  These tag objects enable the `DefaultOrder` and `DefaultScope` template arguments to be deduced via CTAD, as shown in the example below:
[source,c++]
----
auto acc = atomic_accessor(buf, h, relaxed_order, device_scope);
----

==== Sample  Header

[source,c++]
----
namespace cl {
namespace sycl {
namespace intel {

enum class memory_order : /* unspecified */ {
  relaxed, acquire, release, acq_rel, seq_cst
};
inline constexpr memory_order memory_order_relaxed = memory_order::relaxed;
inline constexpr memory_order memory_order_acquire = memory_order::acquire;
inline constexpr memory_order memory_order_release = memory_order::release;
inline constexpr memory_order memory_order_acq_rel = memory_order::acq_rel;
inline constexpr memory_order memory_order_seq_cst = memory_order::seq_cst;

enum class memory_scope : /* unspecified */ {
  work_item, sub_group, work_group, device, system
};
inline constexpr memory_scope memory_scope_work_item = memory_scope::work_item;
inline constexpr memory_scope memory_scope_sub_group = memory_scope::sub_group;
inline constexpr memory_scope memory_scope_work_group = memory_scope::work_group;
inline constexpr memory_scope memory_scope_device = memory_scope::device;
inline constexpr memory_scope memory_scope_system = memory_scope::system;

template <memory_order> struct order_tag_t {
  explicit order_tag_t() = default;
};
inline constexpr order_tag_t<memory_order::relaxed> relaxed_order{};
inline constexpr order_tag_t<memory_order::acquire> acquire_order{};
inline constexpr order_tag_t<memory_order::release> release_order{};
inline constexpr order_tag_t<memory_order::acq_rel> acq_rel_order{};
inline constexpr order_tag_t<memory_order::seq_cst> seq_cst_order{};

template <memory_scope> struct scope_tag_t {
  explicit scope_tag_t() = default;
};
inline constexpr scope_tag_t<memory_scope::work_item> work_item_scope{};
inline constexpr scope_tag_t<memory_scope::sub_group> sub_group_scope{};
inline constexpr scope_tag_t<memory_scope::work_group> work_group_scope{};
inline constexpr scope_tag_t<memory_scope::device> device_scope{};
inline constexpr scope_tag_t<memory_scope::system> system_scope{};

// Exposition only
template <memory_order ReadModifyWriteOrder>
struct memory_order_traits;

template <>
struct memory_order_traits<memory_order::relaxed> {
  static constexpr memory_order read_order = memory_order::relaxed;
  static constexpr memory_order write_order = memory_order::relaxed;
};

template <>
struct memory_order_traits<memory_order::acq_rel> {
  static constexpr memory_order read_order = memory_order::acquire;
  static constexpr memory_order write_order = memory_order::release;
};

template <>
struct memory_order_traits<memory_order::seq_cst> {
  static constexpr memory_order read_order = memory_order::seq_cst;
  static constexpr memory_order write_order = memory_order::seq_cst;
};

template <typename T, memory_order DefaultOrder, memory_scope DefaultScope, access::address_space Space>
class atomic_ref {
 public:

  using value_type = T;
  static constexpr size_t required_alignment = /* implementation-defined */;
  static constexpr bool is_always_lock_free = /* implementation-defined */;
  static constexpr memory_order default_read_order = memory_order_traits<DefaultOrder>::read_order;
  static constexpr memory_order default_write_order = memory_order_traits<DefaultOrder>::write_order;
  static constexpr memory_order default_read_modify_write_order = DefaultOrder;
  static constexpr memory_scope default_scope = DefaultScope;

  bool is_lock_free() const noexcept;

  explicit atomic_ref(T&);
  atomic_ref(const atomic_ref&) noexcept;
  atomic_ref& operator=(const atomic_ref&) = delete;

  void store(T operand,
    memory_order order = default_write_order,
    memory_scope scope = default_scope) const noexcept;

  T operator=(T desired) const noexcept;

  T load(memory_order order = default_read_order,
    memory_scope scope = default_scope) const noexcept;

  operator T() const noexcept;

  T exchange(T operand,
    memory_order order = default_read_modify_write_order,
    memory_scope scope = default_scope) const noexcept;

  bool compare_exchange_weak(T &expected, T desired,
    memory_order success,
    memory_order failure,
    memory_scope scope = default_scope) const noexcept;

  bool compare_exchange_weak(T &expected, T desired,
    memory_order order = default_read_modify_write_order,
    memory_scope scope = default_scope) const noexcept;

  bool compare_exchange_strong(T &expected, T desired,
    memory_order success,
    memory_order failure,
    memory_scope scope = default_scope) const noexcept;

  bool compare_exchange_strong(T &expected, T desired,
    memory_order order = default_read_modify_write_order,
    memory_scope scope = default_scope) const noexcept;
};

// Partial specialization for integral types
template <memory_order DefaultOrder, memory_scope DefaultScope, access::address_space Space>
class atomic_ref<Integral, DefaultOrder, DefaultScope, Space> {

  /* All other members from atomic_ref<T> are available */

  using difference_type = value_type;

  Integral fetch_add(Integral operand,
    memory_order order = default_read_modify_write_order,
    memory_scope scope = default_scope) const noexcept;

  Integral fetch_sub(Integral operand,
    memory_order order = default_read_modify_write_order,
    memory_scope scope = default_scope) const noexcept;

  Integral fetch_and(Integral operand,
    memory_order order = default_read_modify_write_order,
    memory_scope scope = default_scope) const noexcept;

  Integral fetch_or(Integral operand,
    memory_order order = default_read_modify_write_order,
    memory_scope scope = default_scope) const noexcept;

  Integral fetch_min(Integral operand,
    memory_order order = default_read_modify_write_order,
    memory_scope scope = default_scope) const noexcept;

  Integral fetch_max(Integral operand,
    memory_order order = default_read_modify_write_order,
    memory_scope scope = default_scope) const noexcept;

  Integral operator++(int) const noexcept;
  Integral operator--(int) const noexcept;
  Integral operator++() const noexcept;
  Integral operator--() const noexcept;
  Integral operator+=(Integral) const noexcept;
  Integral operator-=(Integral) const noexcept;
  Integral operator&=(Integral) const noexcept;
  Integral operator|=(Integral) const noexcept;
  Integral operator^=(Integral) const noexcept;

};

// Partial specialization for floating-point types
template <memory_order DefaultOrder, memory_scope DefaultScope, access::address_space Space>
class atomic_ref<Floating, DefaultOrder, DefaultScope, Space> {

  /* All other members from atomic_ref<T> are available */

  using difference_type = value_type;

  Floating fetch_add(Floating operand,
    memory_order order = default_read_modify_write_order,
    memory_scope scope = default_scope) const noexcept;

  Floating fetch_sub(Floating operand,
    memory_order order = default_read_modify_write_order,
    memory_scope scope = default_scope) const noexcept;

  Floating fetch_min(Floating operand,
    memory_order order = default_read_modify_write_order,
    memory_scope scope = default_scope) const noexcept;

  Floating fetch_max(Floating operand,
    memory_order order = default_read_modify_write_order,
    memory_scope scope = default_scope) const noexcept;

  Floating operator++(int) const noexcept;
  Floating operator--(int) const noexcept;
  Floating operator++() const noexcept;
  Floating operator--() const noexcept;
  Floating operator+=(Floating) const noexcept;
  Floating operator-=(Floating) const noexcept;

};

// Partial specialization for pointers
template <typename T, memory_order DefaultOrder, memory_scope DefaultScope, access::address_space Space>
class atomic_ref<T*, DefaultOrder, DefaultScope, Space> {

  using value_type = T*;
  using difference_type = ptrdiff_t;
  static constexpr size_t required_alignment = /* implementation-defined */;
  static constexpr bool is_always_lock_free = /* implementation-defined */;
  static constexpr memory_order default_read_order = memory_order_traits<DefaultOrder>::read_order;
  static constexpr memory_order default_write_order = memory_order_traits<DefaultOrder>::write_order;
  static constexpr memory_order default_read_modify_write_order = DefaultOrder;
  static constexpr memory_scope default_scope = DefaultScope;

  bool is_lock_free() const noexcept;

  explicit atomic_ref(T*&);
  atomic_ref(const atomic_ref&) noexcept;
  atomic_ref& operator=(const atomic_ref&) = delete;

  void store(T* operand,
    memory_order order = default_write_order,
    memory_scope scope = default_scope) const noexcept;

  T* operator=(T* desired) const noexcept;

  T* load(memory_order order = default_read_order,
    memory_scope scope = default_scope) const noexcept;

  operator T*() const noexcept;

  T* exchange(T* operand,
    memory_order order = default_read_modify_write_order,
    memory_scope scope = default_scope) const noexcept;

  bool compare_exchange_weak(T* &expected, T* desired,
    memory_order success,
    memory_order failure,
    memory_scope scope = default_scope) const noexcept;

  bool compare_exchange_weak(T* &expected, T* desired,
    memory_order order = default_read_modify_write_order,
    memory_scope scope = default_scope) const noexcept;

  bool compare_exchange_strong(T* &expected, T* desired,
    memory_order success,
    memory_order failure,
    memory_scope scope = default_scope) const noexcept;

  bool compare_exchange_strong(T* &expected, T* desired,
    memory_order order = default_read_modify_write_order,
    memory_scope scope = default_scope) const noexcept;

  T* fetch_add(difference_type,
    memory_order order = default_read_modify_write_order,
    memory_scope scope = default_scope) const noexcept;

  T* fetch_sub(difference_type,
    memory_order order = default_read_modify_write_order,
    memory_scope scope = default_scope) const noexcept;

  T* operator++(int) const noexcept;
  T* operator--(int) const noexcept;
  T* operator++() const noexcept;
  T* operator--() const noexcept;
  T* operator+=(difference_type) const noexcept;
  T* operator-=(difference_type) const noexcept;

};

void atomic_fence(memory_order order, memory_scope scope):

template <typename DataT, int Dimensions,
          memory_order DefaultOrder, memory_scope DefaultScope,
          access::target AccessTarget = access::target::global_buffer,
          access::placeholder IsPlaceholder = access::placeholder::false_t>
class atomic_accessor;

} // namespace intel
} // namespace sycl
} // namespace cl
----

== Issues

None.

//. asd
//+
//--
//*RESOLUTION*: Not resolved.
//--

== Feature test macro

This extension provides a feature-test macro as described in the core SYCL
specification section 6.3.3 "Feature test macros". Therefore, an implementation
supporting this extension must predefine the macro `SYCL_EXT_INTEL_EXTENDED_ATOMICS`
to one of the values defined in the table below. Applications can test for the
existence of this macro to determine if the implementation supports this
feature, or applications can test the macro's value to determine which of the
extension's APIs the implementation supports.

[%header,cols="1,5"]
|===
|Value |Description
|1     |Initial extension version. Base features are supported.
|===

== Revision History

[cols="5,15,15,70"]
[grid="rows"]
[options="header"]
|========================================
|Rev|Date|Author|Changes
|1|2020-01-30|John Pennycook|*Initial public working draft*
|2|2020-04-07|John Pennycook|*Rename class, remove accessor usage, adjust memory orders*
|3|2020-04-09|John Pennycook|*Add atomic_fence*
|4|2020-04-24|John Pennycook|*Add memory scope*
|5|2020-04-29|John Pennycook|*Fix ambiguous overloads of compare_exchange and typo in fetch_sub*
|6|2020-07-08|John Pennycook|*Add atomic_accessor*
|========================================

//************************************************************************
//Other formatting suggestions:
//
//* Use *bold* text for host APIs, or [source] syntax highlighting.
//* Use +mono+ text for device APIs, or [source] syntax highlighting.
//* Use +mono+ text for extension names, types, or enum values.
//* Use _italics_ for parameters.
//************************************************************************
