#
# Configuration Options
#

# Compile flags used to build compute-benchmarks
COMPUTE_BENCH_COMPILE_FLAGS="-j2"
# Number of iterations to run tests for
COMPUTE_BENCH_ITERATIONS="100"

# Metrics to benchmark, and their allowed variance, as a Python dictionary 
#METRICS_VARIANCE='{"Median": 0.5, "StdDev": 4.0}'
METRICS_VARIANCE='{"Median": 0.5}'
# Metrics to record using aggregate.py
METRICS_RECORDED='["Median", "StdDev"]'

# Default period of time to aggregate for the average
AVERAGE_CUTOFF_RANGE="7-days-ago"
# Accepts all valid date strings accepted by GNU coreutils `date` extension:
#
# https://www.gnu.org/software/coreutils/manual/html_node/Date-input-formats.html
#
# Relative timestamps are okay, but replace ' ' with '-', as whitespace gets
# Threshold to store benchmark files before benchmarking
# TODO reconsider this
AVERAGE_THRESHOLD=3
# removed when config file entries are sanitized.

# Enabled ONEAPI_DEVICE_SELECTOR backends
DEVICE_SELECTOR_ENABLED_BACKENDS="level_zero,opencl,cuda,hip"
# Disabled backends: native_cpu

# Enabled ONEAPI_DEVICE_SELECTOR backends
DEVICE_SELECTOR_ENABLED_DEVICES="cpu,gpu"
# Disabled devices: fpga


#
# Constants
#

# Constants used throughout the benchmarking workflow -- do not randomly
# reconfigure

# Github repo + branch settings for repo storing benchmark results
PERF_RES_GIT_REPO="ianayl/llvm-ci-perf-results"
PERF_RES_BRANCH="test-compute-bench"

# Github repo + branch settings for compute-benchmarks itself
COMPUTE_BENCH_GIT_REPO="ianayl/compute-benchmarks"
COMPUTE_BENCH_BRANCH="update-sycl"

# Path to clone benchmark results repo
PERF_RES_PATH="./llvm-ci-perf-res"

# Path to clone and build compute-benchmarks
COMPUTE_BENCH_PATH="./compute-benchmarks"

# Format of timestamps used (unix `date` format string)
TIMESTAMP_FORMAT='%Y%m%d_%H%M%S'

# Path to root folder storing benchmark CI artifact
ARTIFACT_PATH="./artifact"

# Path to temporarily cache compute-benchmark results
OUTPUT_CACHE="./artifact/failed_tests"
# If a test result does not get moved out of this catch-all cache path, it is
# considered to have failed

# Path to cache passing compute-benchmark results
PASSING_CACHE="./artifact/passing_tests"

# Log file for test cases that perform over the allowed variance
BENCHMARK_SLOW_LOG="./artifact/benchmarks_failed.log"
# Log file for test cases that errored / failed to build
BENCHMARK_ERROR_LOG="./artifact/benchmarks_errored.log"
