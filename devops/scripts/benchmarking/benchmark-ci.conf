# Git branch settings for llvm-ci-perf-results
PERF_RES_GIT_REPO="https://github.com/ianayl/llvm-ci-perf-results"
PERF_RES_BRANCH="test-compute-bench"
# Path where llvm-ci-perf-results are cloned
PERF_RES_PATH="./llvm-ci-perf-res"

# Git branch settings for compute-benchmarks
COMPUTE_BENCH_GIT_REPO="https://github.com/ianayl/compute-benchmarks"
COMPUTE_BENCH_BRANCH="update-sycl"

# Path to compile and build compute-benchmarks
COMPUTE_BENCH_PATH="./compute-benchmarks"
# Compile flags used to build compute-benchmarks
COMPUTE_BENCH_COMPILE_FLAGS="-j2"
# Number of iterations to run tests for
COMPUTE_BENCH_ITERATIONS="50"

# Path to temporarily store compute-benchmark results
OUTPUT_PATH="."

# Metrics to benchmark, and their allowed variance as a Python dictionary 
METRICS_VARIANCE='{"Median": 0.5}'
#METRICS_VARIANCE='{"Median": 0.5, "StdDev": 4.0}'

# Metrics to record using aggregate.py
METRICS_RECORDED='["Median", "StdDev"]'

# Threshold to store benchmark files before benchmarking
# TODO reconsider this
AVERAGE_THRESHOLD=3
# Default period of time to aggregate for the average
AVERAGE_CUTOFF_RANGE="7 days ago"

# Format of timestamps used (linux `date` format string)
TIMESTAMP_FORMAT='%Y%m%d_%H%M%S'

# Log file for test cases that perform over the allowed variance
BENCHMARK_SLOW_LOG="./benchmarks-over_tolerance.log"
# Log file for test cases that errored / failed to build
BENCHMARK_ERROR_LOG="./benchmarks-errored.log"
