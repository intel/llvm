; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; Copyright (C) Codeplay Software Limited
;
; Licensed under the Apache License, Version 2.0 (the "License") with LLVM
; Exceptions; you may not use this file except in compliance with the License.
; You may obtain a copy of the License at
;
;     https://github.com/codeplaysoftware/oneapi-construction-kit/blob/main/LICENSE.txt
;
; Unless required by applicable law or agreed to in writing, software
; distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
; WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
; License for the specific language governing permissions and limitations
; under the License.
;
; SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception

; REQUIRES: llvm-13+
; RUN: veczc -vecz-target-triple="riscv64-unknown-unknown" -vecz-scalable -vecz-simd-width=4 -S < %s | FileCheck %s

target triple = "spir64-unknown-unknown"
target datalayout = "e-m:e-i64:64-f80:128-n8:16:32:64-S128"

declare i64 @__mux_get_global_id(i32)

define dso_local spir_kernel void @vector_broadcast_const(<4 x float> addrspace(1)* nocapture readonly %in, <4 x float> addrspace(1)* nocapture %out) local_unnamed_addr #0 {
entry:
  %call = tail call i64 @__mux_get_global_id(i32 0) #6
  %arrayidx = getelementptr inbounds <4 x float>, <4 x float> addrspace(1)* %in, i64 %call
  %0 = bitcast <4 x float> addrspace(1)* %arrayidx to <4 x float> addrspace(1)*
  %1 = load <4 x float>, <4 x float> addrspace(1)* %0, align 16
  %2 = fadd <4 x float> %1, <float 0x7FF0000020000000, float 0x7FF0000020000000, float 0x7FF0000020000000, float 0x7FF0000020000000>
  %arrayidx3 = getelementptr inbounds <4 x float>, <4 x float> addrspace(1)* %out, i64 %call
  store <4 x float> %2, <4 x float> addrspace(1)* %arrayidx3, align 16
  ret void
}

define dso_local spir_kernel void @vector_broadcast(<4 x float> addrspace(1)* nocapture readonly %in, <4 x float> %addend, <4 x float> addrspace(1)* nocapture %out) local_unnamed_addr #0 {
entry:
  %call = tail call i64 @__mux_get_global_id(i32 0) #6
  %arrayidx = getelementptr inbounds <4 x float>, <4 x float> addrspace(1)* %in, i64 %call
  %0 = bitcast <4 x float> addrspace(1)* %arrayidx to <4 x float> addrspace(1)*
  %1 = load <4 x float>, <4 x float> addrspace(1)* %0, align 16
  %2 = fadd <4 x float> %1, %addend
  %arrayidx3 = getelementptr inbounds <4 x float>, <4 x float> addrspace(1)* %out, i64 %call
  store <4 x float> %2, <4 x float> addrspace(1)* %arrayidx3, align 16
  ret void
}

define dso_local spir_kernel void @vector_broadcast_illegal(<32 x float> addrspace(1)* nocapture readonly %in, <32 x float> %addend, <32 x float> addrspace(1)* nocapture %out) local_unnamed_addr #0 {
entry:
  %call = tail call i64 @__mux_get_global_id(i32 0) #6
  %arrayidx = getelementptr inbounds <32 x float>, <32 x float> addrspace(1)* %in, i64 %call
  %0 = bitcast <32 x float> addrspace(1)* %arrayidx to <32 x float> addrspace(1)*
  %1 = load <32 x float>, <32 x float> addrspace(1)* %0, align 64
  %2 = fadd <32 x float> %1, %addend
  %arrayidx3 = getelementptr inbounds <32 x float>, <32 x float> addrspace(1)* %out, i64 %call
  store <32 x float> %2, <32 x float> addrspace(1)* %arrayidx3, align 64
  ret void
}

define dso_local spir_kernel void @vector_broadcast_regression(<4 x float> addrspace(1)* nocapture readonly %in, i32 %nancode, <4 x float> addrspace(1)* nocapture %out) local_unnamed_addr #0 {
entry:
  %call = tail call i64 @__mux_get_global_id(i32 0) #6
  %arrayidx = getelementptr inbounds <4 x float>, <4 x float> addrspace(1)* %in, i64 %call
  %0 = bitcast <4 x float> addrspace(1)* %arrayidx to <4 x i32> addrspace(1)*
  %1 = load <4 x i32>, <4 x i32> addrspace(1)* %0, align 16
  %and1.i.i.i1.i = and <4 x i32> %1, <i32 2139095040, i32 2139095040, i32 2139095040, i32 2139095040>
  %cmp.i.i.i2.i = icmp ne <4 x i32> %and1.i.i.i1.i, <i32 2139095040, i32 2139095040, i32 2139095040, i32 2139095040>
  %and2.i.i.i3.i = and <4 x i32> %1, <i32 8388607, i32 8388607, i32 8388607, i32 8388607>
  %cmp3.i.i.i4.i = icmp eq <4 x i32> %and2.i.i.i3.i, zeroinitializer
  %2 = or <4 x i1> %cmp.i.i.i2.i, %cmp3.i.i.i4.i
  %3 = bitcast <4 x i32> %1 to <4 x float>
  %4 = select <4 x i1> %2, <4 x float> %3, <4 x float> <float 0x7FF0000020000000, float 0x7FF0000020000000, float 0x7FF0000020000000, float 0x7FF0000020000000>
  %arrayidx3 = getelementptr inbounds <4 x float>, <4 x float> addrspace(1)* %out, i64 %call
  store <4 x float> %4, <4 x float> addrspace(1)* %arrayidx3, align 16
  ret void
}

; Check that new instructions aren't inserting before pre-existing allocas
define dso_local spir_kernel void @vector_broadcast_insertpt(<4 x float> addrspace(1)* nocapture readonly %in, <4 x float> %addend, i32 %nancode, <4 x float> addrspace(1)* nocapture %out, <4 x i32> addrspace(1)* nocapture %out2) local_unnamed_addr #0 {
entry:
  %existing.alloc = alloca <4 x i32>
  %call = tail call i64 @__mux_get_global_id(i32 0) #6
  store <4 x i32> zeroinitializer, <4 x i32>* %existing.alloc
  %scalar = bitcast <4 x i32>* %existing.alloc to i32*
  store i32 1, i32* %scalar
  %v = load <4 x i32>, <4 x i32>* %existing.alloc
  %arrayidx4 = getelementptr inbounds <4 x i32>, <4 x i32> addrspace(1)* %out2, i64 %call
  store <4 x i32> %v, <4 x i32> addrspace(1)* %arrayidx4, align 16

  %arrayidx = getelementptr inbounds <4 x float>, <4 x float> addrspace(1)* %in, i64 %call
  %op = load <4 x float>, <4 x float> addrspace(1)* %arrayidx, align 16
  %v4 = fadd <4 x float> %op, %addend
  %arrayidx3 = getelementptr inbounds <4 x float>, <4 x float> addrspace(1)* %out, i64 %call
  store <4 x float> %v4, <4 x float> addrspace(1)* %arrayidx3, align 16
  ret void
}

define dso_local spir_kernel void @vector_mask_broadcast(<4 x float> addrspace(1)* nocapture readonly %in, <4 x i1> %input, <4 x float> %woof, <4 x float> addrspace(1)* nocapture %out) local_unnamed_addr #0 {
entry:
  %call = tail call i64 @__mux_get_global_id(i32 0) #6
  %arrayidx = getelementptr inbounds <4 x float>, <4 x float> addrspace(1)* %in, i64 %call
  %0 = bitcast <4 x float> addrspace(1)* %arrayidx to <4 x float> addrspace(1)*
  %1 = load <4 x float>, <4 x float> addrspace(1)* %0, align 16
  %2 = fcmp oeq <4 x float> %1, <float 1.0, float 1.0, float 1.0, float 1.0>
  %3 = and <4 x i1> %2, %input
  %4 = select <4 x i1> %3, <4 x float> %1, <4 x float> %woof
  %arrayidx3 = getelementptr inbounds <4 x float>, <4 x float> addrspace(1)* %out, i64 %call
  store <4 x float> %4, <4 x float> addrspace(1)* %arrayidx3, align 16
  ret void
}
; CHECK-LABEL: @__vecz_nxv4_vector_broadcast_const(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[CALL:%.*]] = tail call i64 @__mux_get_global_id(i32 0)
; CHECK-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <4 x float>, ptr addrspace(1) [[OUT:%.*]], i64 [[CALL]]
; CHECK-NEXT:    store <vscale x 16 x float> shufflevector (<vscale x 16 x float> insertelement (<vscale x 16 x float> {{(undef|poison)}}, float 0x7FF0000020000000, {{i32|i64}} 0), <vscale x 16 x float> {{(undef|poison)}}, <vscale x 16 x i32> zeroinitializer), ptr addrspace(1) [[ARRAYIDX3]], align 16
; CHECK-NEXT:    ret void
;
; CHECK-LABEL: @__vecz_nxv4_vector_broadcast(
; CHECK-NEXT:  entry:
; CHECK-NEXT:  [[VS2:%.*]] = call <vscale x 16 x float> @llvm.{{(experimental.)?}}vector.insert.nxv16f32.v4f32(<vscale x 16 x float> poison, <4 x float> [[ADDEND:%.*]], i64 0)
; CHECK-NEXT:  [[IDX0:%.*]] = call <vscale x 16 x i32> @llvm.experimental.stepvector.nxv16i32()
; CHECK-NEXT:  [[VS1:%.*]] = and <vscale x 16 x i32> [[IDX0]], shufflevector (<vscale x 16 x i32> insertelement (<vscale x 16 x i32> {{(undef|poison)}}, i32 3, {{i32|i64}} 0), <vscale x 16 x i32> {{(undef|poison)}}, <vscale x 16 x i32> zeroinitializer)
; CHECK-NEXT:  [[XLEN:%.*]] = call i64 @llvm.vscale.i64()
; CHECK-NEXT:  [[TMP0:%.*]] = shl i64 [[XLEN]], 4
; CHECK-NEXT:  [[TMP1:%.*]] = call <vscale x 16 x float> @llvm.riscv.vrgather.vv.nxv16f32.i64(<vscale x 16 x float> undef, <vscale x 16 x float> [[VS2]], <vscale x 16 x i32> [[VS1]], i64 [[TMP0]])
; CHECK-NEXT:  [[CALL:%.*]] = tail call i64 @__mux_get_global_id(i32 0)
; CHECK-NEXT:  [[ARRAYIDX:%.*]] = getelementptr inbounds <4 x float>, ptr addrspace(1) [[IN:%.*]], i64 [[CALL]]
; CHECK-NEXT:  [[TMP3:%.*]] = load <vscale x 16 x float>, ptr addrspace(1) [[ARRAYIDX]], align 4
; CHECK-NEXT:  [[TMP4:%.*]] = fadd <vscale x 16 x float> [[TMP3]], [[TMP1]]
; CHECK-NEXT:  [[ARRAYIDX3:%.*]] = getelementptr inbounds <4 x float>, ptr addrspace(1) [[OUT:%.*]], i64 [[CALL]]
; CHECK-NEXT:  store <vscale x 16 x float> [[TMP4]], ptr addrspace(1) [[ARRAYIDX3]], align 16
; CHECK-NEXT:  ret void
;
; CHECK-LABEL: @__vecz_nxv4_vector_broadcast_illegal(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[FIXLEN_ALLOC:%.*]] = alloca <32 x float>, align 128
; CHECK-NEXT:    store <32 x float> [[ADDEND:%.*]], ptr [[FIXLEN_ALLOC]], align 128
; CHECK-NEXT:    [[IDX0:%.*]] = call <vscale x 128 x i32> @llvm.experimental.stepvector.nxv128i32()
; CHECK-NEXT:    [[IDX1:%.*]] = and <vscale x 128 x i32> [[IDX0]], shufflevector (<vscale x 128 x i32> insertelement (<vscale x 128 x i32> {{(undef|poison)}}, i32 31, {{i32|i64}} 0), <vscale x 128 x i32> {{(undef|poison)}}, <vscale x 128 x i32> zeroinitializer)
; CHECK-NEXT:    [[TMP0:%.*]] = {{s|z}}ext{{( nneg)?}} <vscale x 128 x i32> [[IDX1]] to <vscale x 128 x i64>
; CHECK-NEXT:    [[VEC_ALLOC:%.*]] = getelementptr inbounds float, ptr [[FIXLEN_ALLOC]], <vscale x 128 x i64> [[TMP0]]
; CHECK-NEXT:    [[TMP1:%.*]] = call <vscale x 128 x float> @llvm.masked.gather.nxv128f32.nxv128p0(<vscale x 128 x ptr> [[VEC_ALLOC]], i32 4, <vscale x 128 x i1> shufflevector (<vscale x 128 x i1> insertelement (<vscale x 128 x i1> poison, i1 true, {{i32|i64}} 0), <vscale x 128 x i1> poison, <vscale x 128 x i32> zeroinitializer), <vscale x 128 x float> undef)
; CHECK-NEXT:    [[CALL:%.*]] = tail call i64 @__mux_get_global_id(i32 0)
; CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <32 x float>, ptr addrspace(1) [[IN:%.*]], i64 [[CALL]]
; CHECK-NEXT:    [[TMP3:%.*]] = load <vscale x 128 x float>, ptr addrspace(1) [[ARRAYIDX]], align 4
; CHECK-NEXT:    [[TMP4:%.*]] = fadd <vscale x 128 x float> [[TMP3]], [[TMP1]]
; CHECK-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <32 x float>, ptr addrspace(1) [[OUT:%.*]], i64 [[CALL]]
; CHECK-NEXT:    store <vscale x 128 x float> [[TMP4]], ptr addrspace(1) [[ARRAYIDX3]], align 64
; CHECK-NEXT:    ret void
;
; CHECK-LABEL: @__vecz_nxv4_vector_broadcast_regression(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[CALL:%.*]] = tail call i64 @__mux_get_global_id(i32 0)
; CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <4 x float>, ptr addrspace(1) [[IN:%.*]], i64 [[CALL]]
; CHECK-NEXT:    [[TMP1:%.*]] = load <vscale x 16 x i32>, ptr addrspace(1) [[ARRAYIDX]], align 4
; CHECK-NEXT:    [[AND1_I_I_I1_I1:%.*]] = and <vscale x 16 x i32> [[TMP1]], shufflevector (<vscale x 16 x i32> insertelement (<vscale x 16 x i32> {{(undef|poison)}}, i32 2139095040, {{i32|i64}} 0), <vscale x 16 x i32> {{(undef|poison)}}, <vscale x 16 x i32> zeroinitializer)
; CHECK-NEXT:    [[CMP_I_I_I2_I2:%.*]] = icmp ne <vscale x 16 x i32> [[AND1_I_I_I1_I1]], shufflevector (<vscale x 16 x i32> insertelement (<vscale x 16 x i32> {{(undef|poison)}}, i32 2139095040, {{i32|i64}} 0), <vscale x 16 x i32> {{(undef|poison)}}, <vscale x 16 x i32> zeroinitializer)
; CHECK-NEXT:    [[AND2_I_I_I3_I3:%.*]] = and <vscale x 16 x i32> [[TMP1]], shufflevector (<vscale x 16 x i32> insertelement (<vscale x 16 x i32> {{(undef|poison)}}, i32 8388607, {{i32|i64}} 0), <vscale x 16 x i32> {{(undef|poison)}}, <vscale x 16 x i32> zeroinitializer)
; CHECK-NEXT:    [[CMP3_I_I_I4_I4:%.*]] = icmp eq <vscale x 16 x i32> [[AND2_I_I_I3_I3]], zeroinitializer
; CHECK-NEXT:    [[TMP2:%.*]] = or <vscale x 16 x i1> [[CMP_I_I_I2_I2]], [[CMP3_I_I_I4_I4]]
; CHECK-NEXT:    [[TMP3:%.*]] = bitcast <vscale x 16 x i32> [[TMP1]] to <vscale x 16 x float>
; CHECK-NEXT:    [[TMP4:%.*]] = select <vscale x 16 x i1> [[TMP2]], <vscale x 16 x float> [[TMP3]], <vscale x 16 x float> shufflevector (<vscale x 16 x float> insertelement (<vscale x 16 x float> {{(undef|poison)}}, float 0x7FF0000020000000, {{i32|i64}} 0), <vscale x 16 x float> {{(undef|poison)}}, <vscale x 16 x i32> zeroinitializer)
; CHECK-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds <4 x float>, ptr addrspace(1) [[OUT:%.*]], i64 [[CALL]]
; CHECK-NEXT:    store <vscale x 16 x float> [[TMP4]], ptr addrspace(1) [[ARRAYIDX3]], align 16
; CHECK-NEXT:    ret void
;
;
; CHECK-LABEL: @__vecz_nxv4_vector_broadcast_insertpt(
; CHECK-NEXT: entry:
; CHECK-NEXT:  [[EXISTINGALLOC:%.*]] = alloca <4 x i32>, align 16
; CHECK-NEXT:  [[VS21:%.*]] = call <vscale x 16 x float> @llvm.{{(experimental.)?}}vector.insert.nxv16f32.v4f32(<vscale x 16 x float> poison, <4 x float> [[ADDEND:%.*]], i64 0)
; CHECK-NEXT:  [[IDX02:%.*]] = call <vscale x 16 x i32> @llvm.experimental.stepvector.nxv16i32()
; CHECK-NEXT:  [[VS13:%.*]] = and <vscale x 16 x i32> [[IDX02]], shufflevector (<vscale x 16 x i32> insertelement (<vscale x 16 x i32> {{(undef|poison)}}, i32 3, {{i32|i64}} 0), <vscale x 16 x i32> {{(undef|poison)}}, <vscale x 16 x i32> zeroinitializer)
; CHECK-NEXT:  [[XLEN:%.*]] = call i64 @llvm.vscale.i64()
; CHECK-NEXT:  [[TMP0:%.*]] = shl i64 [[XLEN]], 4
; CHECK-NEXT:  [[TMP1:%.*]] = call <vscale x 16 x float> @llvm.riscv.vrgather.vv.nxv16f32.i64(<vscale x 16 x float> undef, <vscale x 16 x float> [[VS21]], <vscale x 16 x i32> [[VS13]], i64 [[TMP0]])
; CHECK-NEXT:  [[CALL:%.*]] = tail call i64 @__mux_get_global_id(i32 0)
; CHECK-NEXT:  store <4 x i32> zeroinitializer, ptr [[EXISTINGALLOC]], align 16
; CHECK-NEXT:  store i32 1, ptr [[EXISTINGALLOC]], align 16
; CHECK-NEXT:  [[V:%.*]] = load <4 x i32>, ptr [[EXISTINGALLOC]], align 16
; CHECK-NEXT:  [[VS2:%.*]] = call <vscale x 16 x i32> @llvm.{{(experimental.)?}}vector.insert.nxv16i32.v4i32(<vscale x 16 x i32> poison, <4 x i32> [[V]], i64 0)
; CHECK-NEXT:  [[TMP2:%.*]] = call <vscale x 16 x i32> @llvm.riscv.vrgather.vv.nxv16i32.i64(<vscale x 16 x i32> undef, <vscale x 16 x i32> [[VS2]], <vscale x 16 x i32> [[VS13]], i64 [[TMP0]])
; CHECK-NEXT:  [[ARRAYIDX4:%.*]] = getelementptr inbounds <4 x i32>, ptr addrspace(1) [[OUT2:%.*]], i64 [[CALL]]
; CHECK-NEXT:  store <vscale x 16 x i32> [[TMP2]], ptr addrspace(1) [[ARRAYIDX4]], align 16
; CHECK-NEXT:  [[ARRAYIDX:%.*]] = getelementptr inbounds <4 x float>, ptr addrspace(1) [[IN:%.*]], i64 [[CALL]]
; CHECK-NEXT:  [[TMP5:%.*]] = load <vscale x 16 x float>, ptr addrspace(1) [[ARRAYIDX]], align 4
; CHECK-NEXT:  [[V44:%.*]] = fadd <vscale x 16 x float> [[TMP5]], [[TMP1]]
; CHECK-NEXT:  [[ARRAYIDX3:%.*]] = getelementptr inbounds <4 x float>, ptr addrspace(1) [[OUT:%.*]], i64 [[CALL]]
; CHECK-NEXT:  store <vscale x 16 x float> [[V44]], ptr addrspace(1) [[ARRAYIDX3]], align 16
; CHECK-NEXT:  ret void
;
; CHECK-LABEL: @__vecz_nxv4_vector_mask_broadcast(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[VS21:%.*]] = call <vscale x 16 x float> @llvm.{{(experimental.)?}}vector.insert.nxv16f32.v4f32(<vscale x 16 x float> poison, <4 x float> [[WOOF:%.*]], i64 0)
; CHECK-NEXT:    [[IDX02:%.*]] = call <vscale x 16 x i32> @llvm.experimental.stepvector.nxv16i32()
; CHECK-NEXT:    [[VS13:%.*]] = and <vscale x 16 x i32> [[IDX02]], shufflevector (<vscale x 16 x i32> insertelement (<vscale x 16 x i32> {{(undef|poison)}}, i32 3, {{i32|i64}} 0), <vscale x 16 x i32> {{(undef|poison)}}, <vscale x 16 x i32> zeroinitializer)
; CHECK-NEXT:    [[XLEN4:%.*]] = call i64 @llvm.vscale.i64()
; CHECK-NEXT:    [[TMP0:%.*]] = shl i64 [[XLEN4]], 4
; CHECK-NEXT:    [[TMP1:%.*]] = call <vscale x 16 x float> @llvm.riscv.vrgather.vv.nxv16f32.i64(<vscale x 16 x float> undef, <vscale x 16 x float> [[VS21]], <vscale x 16 x i32> [[VS13]], i64 [[TMP0]])
; CHECK-NEXT:    [[TMP2:%.*]] = sext <4 x i1> [[INPUT:%.*]] to <4 x i8>
; CHECK-NEXT:    [[VS2:%.*]] = call <vscale x 16 x i8> @llvm.{{(experimental.)?}}vector.insert.nxv16i8.v4i8(<vscale x 16 x i8> poison, <4 x i8> [[TMP2]], i64 0)
; CHECK-NEXT:    [[IDX0:%.*]] = call <vscale x 16 x i16> @llvm.experimental.stepvector.nxv16i16()
; CHECK-NEXT:    [[VS1:%.*]] = and <vscale x 16 x i16> [[IDX0]], shufflevector (<vscale x 16 x i16> insertelement (<vscale x 16 x i16> {{(undef|poison)}}, i16 3, {{i32|i64}} 0), <vscale x 16 x i16> {{(undef|poison)}}, <vscale x 16 x i32> zeroinitializer)
; CHECK-NEXT:    [[TMP3:%.*]] = call <vscale x 16 x i8> @llvm.riscv.vrgatherei16.vv.nxv16i8.i64(<vscale x 16 x i8> undef, <vscale x 16 x i8> [[VS2]], <vscale x 16 x i16> [[VS1]], i64 [[TMP0]])
; CHECK: [[TMP4:%.*]] = trunc <vscale x 16 x i8> [[TMP3]] to <vscale x 16 x i1>
; CHECK: [[TMP5:%.*]] = fcmp oeq <vscale x 16 x float>
; CHECK: [[TMP8:%.*]] = and <vscale x 16 x i1> [[TMP5]], [[TMP4]]
