; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; RUN: opt -x86-lower-matrix-operation -mtriple=x86_64-unknown-unknown -mcpu=sapphirerapids %s -S | FileCheck %s

define void @test_load_store_different_addrspace(i32 addrspace(4)* %ptr, i64 %stride, i32 addrspace(4)* %dst) {
; CHECK-LABEL: @test_load_store_different_addrspace(
; CHECK-NEXT:  enrty:
; CHECK-NEXT:    [[TMP0:%.*]] = addrspacecast i32 addrspace(4)* [[PTR:%.*]] to i8*
; CHECK-NEXT:    [[TMP1:%.*]] = mul i64 [[STRIDE:%.*]], 1
; CHECK-NEXT:    [[TMP2:%.*]] = call x86_amx @llvm.x86.tileloadd64.internal(i16 1, i16 8, i8* [[TMP0]], i64 [[TMP1]])
; CHECK-NEXT:    [[TMP3:%.*]] = call <8 x i8> @llvm.x86.cast.tile.to.vector.v8i8(x86_amx [[TMP2]])
; CHECK-NEXT:    [[TMP4:%.*]] = addrspacecast i32 addrspace(4)* [[DST:%.*]] to i8*
; CHECK-NEXT:    [[TMP5:%.*]] = mul i64 [[STRIDE]], 1
; CHECK-NEXT:    [[TMP6:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v8i8(<8 x i8> [[TMP3]])
; CHECK-NEXT:    call void @llvm.x86.tilestored64.internal(i16 1, i16 8, i8* [[TMP4]], i64 [[TMP5]], x86_amx [[TMP6]])
; CHECK-NEXT:    ret void
;
enrty:
  %0 = call <8 x i8> @llvm.experimental.matrix.load.v8i8.p4i8(i32 addrspace(4)* %ptr,  i64 %stride, i1 false, i32 4, i32 2, metadata !"matrix.packed.b", metadata !"matrix.packed.b", metadata !"scope.subgroup")
  call void @llvm.experimental.matrix.store.v8i8.p4i8(<8 x i8> %0, i32 addrspace(4)* %dst,  i64 %stride, i1 false, i32 4, i32 2, metadata !"matrix.packed.b", metadata !"matrix.packed.b", metadata !"scope.subgroup")
  ret void
}

define void @test_load_store_same_addrspace(i32* %ptr, i64 %stride, i32* %dst) {
; CHECK-LABEL: @test_load_store_same_addrspace(
; CHECK-NEXT:  enrty:
; CHECK-NEXT:    [[TMP0:%.*]] = bitcast i32* [[PTR:%.*]] to i8*
; CHECK-NEXT:    [[TMP1:%.*]] = mul i64 [[STRIDE:%.*]], 1
; CHECK-NEXT:    [[TMP2:%.*]] = call x86_amx @llvm.x86.tileloadd64.internal(i16 1, i16 8, i8* [[TMP0]], i64 [[TMP1]])
; CHECK-NEXT:    [[TMP3:%.*]] = call <8 x i8> @llvm.x86.cast.tile.to.vector.v8i8(x86_amx [[TMP2]])
; CHECK-NEXT:    [[TMP4:%.*]] = bitcast i32* [[DST:%.*]] to i8*
; CHECK-NEXT:    [[TMP5:%.*]] = mul i64 [[STRIDE]], 1
; CHECK-NEXT:    [[TMP6:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v8i8(<8 x i8> [[TMP3]])
; CHECK-NEXT:    call void @llvm.x86.tilestored64.internal(i16 1, i16 8, i8* [[TMP4]], i64 [[TMP5]], x86_amx [[TMP6]])
; CHECK-NEXT:    ret void
;
enrty:
  %0 = call <8 x i8> @llvm.experimental.matrix.load.v8i8.p4i8.v2(i32* %ptr,  i64 %stride, i1 false, i32 4, i32 2, metadata !"matrix.packed.b", metadata !"matrix.packed.b", metadata !"scope.subgroup")
  call void @llvm.experimental.matrix.store.v8i8.p4i8.v2(<8 x i8> %0, i32* %dst,  i64 %stride, i1 false, i32 4, i32 2, metadata !"matrix.packed.b", metadata !"matrix.packed.b", metadata !"scope.subgroup")
  ret void
}

define void @load_mad_store_int8(i32 addrspace(4)* %ptr, i64 %Stride) {
; CHECK-LABEL: @load_mad_store_int8(
; CHECK-NEXT:    [[TMP1:%.*]] = addrspacecast i32 addrspace(4)* [[PTR:%.*]] to i8*
; CHECK-NEXT:    [[TMP2:%.*]] = mul i64 [[STRIDE:%.*]], 1
; CHECK-NEXT:    [[TMP3:%.*]] = call x86_amx @llvm.x86.tileloadd64.internal(i16 3, i16 4, i8* [[TMP1]], i64 [[TMP2]])
; CHECK-NEXT:    [[TMP4:%.*]] = call <12 x i8> @llvm.x86.cast.tile.to.vector.v12i8(x86_amx [[TMP3]])
; CHECK-NEXT:    [[TMP5:%.*]] = addrspacecast i32 addrspace(4)* [[PTR]] to i8*
; CHECK-NEXT:    [[TMP6:%.*]] = mul i64 [[STRIDE]], 1
; CHECK-NEXT:    [[TMP7:%.*]] = call x86_amx @llvm.x86.tileloadd64.internal(i16 1, i16 20, i8* [[TMP5]], i64 [[TMP6]])
; CHECK-NEXT:    [[TMP8:%.*]] = call <20 x i8> @llvm.x86.cast.tile.to.vector.v20i8(x86_amx [[TMP7]])
; CHECK-NEXT:    [[TMP9:%.*]] = addrspacecast i32 addrspace(4)* [[PTR]] to i8*
; CHECK-NEXT:    [[TMP10:%.*]] = mul i64 [[STRIDE]], 4
; CHECK-NEXT:    [[TMP11:%.*]] = call x86_amx @llvm.x86.tileloadd64.internal(i16 3, i16 20, i8* [[TMP9]], i64 [[TMP10]])
; CHECK-NEXT:    [[TMP12:%.*]] = call <15 x i32> @llvm.x86.cast.tile.to.vector.v15i32(x86_amx [[TMP11]])
; CHECK-NEXT:    [[TMP13:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v15i32(<15 x i32> [[TMP12]])
; CHECK-NEXT:    [[TMP14:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v12i8(<12 x i8> [[TMP4]])
; CHECK-NEXT:    [[TMP15:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v20i8(<20 x i8> [[TMP8]])
; CHECK-NEXT:    [[TMP16:%.*]] = call x86_amx @llvm.x86.tdpbssd.internal(i16 3, i16 20, i16 4, x86_amx [[TMP13]], x86_amx [[TMP14]], x86_amx [[TMP15]])
; CHECK-NEXT:    [[TMP17:%.*]] = call <15 x i32> @llvm.x86.cast.tile.to.vector.v15i32(x86_amx [[TMP16]])
; CHECK-NEXT:    [[TMP18:%.*]] = addrspacecast i32 addrspace(4)* [[PTR]] to i8*
; CHECK-NEXT:    [[TMP19:%.*]] = mul i64 [[STRIDE]], 4
; CHECK-NEXT:    [[TMP20:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v15i32(<15 x i32> [[TMP17]])
; CHECK-NEXT:    call void @llvm.x86.tilestored64.internal(i16 3, i16 20, i8* [[TMP18]], i64 [[TMP19]], x86_amx [[TMP20]])
; CHECK-NEXT:    ret void
;
  %A = call <12 x i8> @llvm.experimental.matrix.load.v12i8(i32 addrspace(4)* %ptr, i64 %Stride, i1 false, i32 3, i32 4, metadata !"matrix.rowmajor", metadata !"matrix.rowmajor", metadata !"scope.subgroup")
  %B = call <20 x i8> @llvm.experimental.matrix.load.v20i8(i32 addrspace(4)* %ptr, i64 %Stride, i1 false, i32 4, i32 5, metadata !"matrix.packed.b", metadata !"matrix.packed.b", metadata !"scope.subgroup")
  %C = call <15 x i32> @llvm.experimental.matrix.load.v15i32(i32 addrspace(4)* %ptr, i64 %Stride, i1 false, i32 3, i32 5, metadata !"matrix.rowmajor", metadata !"matrix.rowmajor", metadata !"scope.subgroup")
  %D = call <15 x i32> @llvm.experimental.matrix.mad.v12i32.v20i32.v15i32(<12 x i8> %A, metadata !"matrix.rowmajor", <20 x i8> %B, metadata !"matrix.packed.b", <15 x i32> %C, metadata !"matrix.rowmajor", i32 3, i32 4, i32 5, metadata !"scope.subgroup")
  call void @llvm.experimental.matrix.store.v15i32(<15 x i32> %D, i32 addrspace(4)* %ptr, i64 %Stride, i1 0, i32 3, i32 5, metadata !"matrix.rowmajor", metadata !"matrix.rowmajor", metadata !"scope.subgroup")
  ret void
}

define void @load_mad_store_bf16(i16 addrspace(4)* %ptrA, i16 addrspace(4)* %ptrB, float addrspace(4)* %ptrC, float addrspace(4)* %ptrD, i64 %Stride) {
; CHECK-LABEL: @load_mad_store_bf16(
; CHECK-NEXT:    [[TMP1:%.*]] = addrspacecast i16 addrspace(4)* [[PTRA:%.*]] to i8*
; CHECK-NEXT:    [[TMP2:%.*]] = mul i64 [[STRIDE:%.*]], 2
; CHECK-NEXT:    [[TMP3:%.*]] = call x86_amx @llvm.x86.tileloadd64.internal(i16 15, i16 60, i8* [[TMP1]], i64 [[TMP2]])
; CHECK-NEXT:    [[TMP4:%.*]] = call <450 x i16> @llvm.x86.cast.tile.to.vector.v450i16(x86_amx [[TMP3]])
; CHECK-NEXT:    [[TMP5:%.*]] = addrspacecast i16 addrspace(4)* [[PTRB:%.*]] to i8*
; CHECK-NEXT:    [[TMP6:%.*]] = mul i64 [[STRIDE]], 2
; CHECK-NEXT:    [[TMP7:%.*]] = call x86_amx @llvm.x86.tileloadd64.internal(i16 15, i16 60, i8* [[TMP5]], i64 [[TMP6]])
; CHECK-NEXT:    [[TMP8:%.*]] = call <450 x i16> @llvm.x86.cast.tile.to.vector.v450i16(x86_amx [[TMP7]])
; CHECK-NEXT:    [[TMP9:%.*]] = addrspacecast float addrspace(4)* [[PTRC:%.*]] to i8*
; CHECK-NEXT:    [[TMP10:%.*]] = mul i64 [[STRIDE]], 4
; CHECK-NEXT:    [[TMP11:%.*]] = call x86_amx @llvm.x86.tileloadd64.internal(i16 15, i16 60, i8* [[TMP9]], i64 [[TMP10]])
; CHECK-NEXT:    [[TMP12:%.*]] = call <225 x float> @llvm.x86.cast.tile.to.vector.v225f32(x86_amx [[TMP11]])
; CHECK-NEXT:    [[TMP13:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v225f32(<225 x float> [[TMP12]])
; CHECK-NEXT:    [[TMP14:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v450i16(<450 x i16> [[TMP4]])
; CHECK-NEXT:    [[TMP15:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v450i16(<450 x i16> [[TMP8]])
; CHECK-NEXT:    [[TMP16:%.*]] = call x86_amx @llvm.x86.tdpbf16ps.internal(i16 15, i16 60, i16 60, x86_amx [[TMP13]], x86_amx [[TMP14]], x86_amx [[TMP15]])
; CHECK-NEXT:    [[TMP17:%.*]] = call <225 x float> @llvm.x86.cast.tile.to.vector.v225f32(x86_amx [[TMP16]])
; CHECK-NEXT:    [[TMP18:%.*]] = addrspacecast float addrspace(4)* [[PTRD:%.*]] to i8*
; CHECK-NEXT:    [[TMP19:%.*]] = mul i64 [[STRIDE]], 4
; CHECK-NEXT:    [[TMP20:%.*]] = call x86_amx @llvm.x86.cast.vector.to.tile.v225f32(<225 x float> [[TMP17]])
; CHECK-NEXT:    call void @llvm.x86.tilestored64.internal(i16 15, i16 60, i8* [[TMP18]], i64 [[TMP19]], x86_amx [[TMP20]])
; CHECK-NEXT:    ret void
;
  %A = call <450 x i16> @llvm.experimental.matrix.load.v450i16.p4i16(i16 addrspace(4)* %ptrA, i64 %Stride, i1 false, i32 15, i32 30, metadata !"matrix.rowmajor", metadata !"matrix.rowmajor", metadata !"scope.subgroup")
  %B = call <450 x i16> @llvm.experimental.matrix.load.v450i16.p4i16(i16 addrspace(4)* %ptrB, i64 %Stride, i1 false, i32 30, i32 15, metadata !"matrix.packed.b", metadata !"matrix.packed.b", metadata !"scope.subgroup")
  %C = call <225 x float> @llvm.experimental.matrix.load.v225f32.p4f32(float addrspace(4)* %ptrC, i64 %Stride, i1 false, i32 15, i32 15, metadata !"matrix.rowmajor", metadata !"matrix.rowmajor", metadata !"scope.subgroup")
  %D = call <225 x float> @llvm.experimental.matrix.mad.v225f32.v450i16.v450i16(<450 x i16> %A, metadata !"matrix.rowmajor", <450 x i16> %B, metadata !"matrix.packed.b", <225 x float> %C, metadata !"matrix.rowmajor", i32 15, i32 30, i32 15, metadata !"scope.subgroup")
  call void @llvm.experimental.matrix.store.v225f32.p4f32(<225 x float> %D, float addrspace(4)* %ptrD, i64 %Stride, i1 false, i32 15, i32 15, metadata !"matrix.rowmajor", metadata !"matrix.rowmajor", metadata !"scope.subgroup")
  ret void
}

declare <12 x i8> @llvm.experimental.matrix.load.v12i8(i32 addrspace(4)*, i64, i1, i32, i32, metadata, metadata, metadata);
declare <20 x i8> @llvm.experimental.matrix.load.v20i8(i32 addrspace(4)*, i64, i1, i32, i32, metadata, metadata, metadata);
declare <15 x i32> @llvm.experimental.matrix.load.v15i32(i32 addrspace(4)*, i64, i1, i32, i32, metadata, metadata, metadata);
declare <15 x i32> @llvm.experimental.matrix.mad.v12i32.v20i32.v15i32(<12 x i8>, metadata, <20 x i8>, metadata, <15 x i32>, metadata, i32, i32, i32, metadata)
declare void @llvm.experimental.matrix.store.v15i32(<15 x i32>, i32 addrspace(4)*, i64, i1, i32, i32, metadata, metadata, metadata);

declare <8 x i8> @llvm.experimental.matrix.load.v8i8.p4i8(i32 addrspace(4)*, i64, i1, i32, i32, metadata, metadata, metadata);
declare void @llvm.experimental.matrix.store.v8i8.p4i8(<8 x i8>, i32 addrspace(4)*, i64, i1, i32, i32, metadata, metadata, metadata);
declare <8 x i8> @llvm.experimental.matrix.load.v8i8.p4i8.v2(i32*, i64, i1, i32, i32, metadata, metadata, metadata);
declare void @llvm.experimental.matrix.store.v8i8.p4i8.v2(<8 x i8>, i32*, i64, i1, i32, i32, metadata, metadata, metadata);

declare <225 x float> @llvm.experimental.matrix.load.v225f32.p4f32(float addrspace(4)*, i64, i1, i32, i32, metadata, metadata, metadata)
declare <450 x i16> @llvm.experimental.matrix.load.v450i16.p4i16(i16 addrspace(4)*, i64, i1, i32, i32, metadata, metadata, metadata)
declare <225 x float> @llvm.experimental.matrix.mad.v225f32.v450i16.v450i16(<450 x i16>, metadata, <450 x i16>, metadata, <225 x float>, metadata, i32, i32, i32, metadata)
declare void @llvm.experimental.matrix.store.v225f32.p4f32(<225 x float>, float addrspace(4)*, i64, i1, i32, i32, metadata, metadata, metadata)

