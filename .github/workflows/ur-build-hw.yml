name: UR - Build adapters, test on HW

on:
  workflow_call:
    inputs:
      adapter_name:
        required: true
        type: string
      other_adapter_name:
        required: false
        type: string
        default: ""
      runner_name:
        required: true
        type: string
      platform:
        description: "Platform string, `UR_CTS_ADAPTER_PLATFORM` will be set to this."
        required: false
        type: string
        default: ""
      static_loader:
        required: false
        type: string
        default: OFF
      static_adapter:
        required: false
        type: string
        default: OFF
      docker_image:
        required: true
        type: string
        default: ""
  workflow_dispatch:
    inputs:
      adapter_name:
        required: true
        type: string
      other_adapter_name:
        required: false
        type: string
        default: ""
      runner_name:
        required: true
        type: string
      platform:
        description: "Platform string, `UR_CTS_ADAPTER_PLATFORM` will be set to this."
        required: false
        type: string
        default: ""
      static_loader:
        required: false
        type: string
        default: OFF
      static_adapter:
        required: false
        type: string
        default: OFF
      docker_image:
        required: true
        type: string
        default: ""

permissions: read-all

env:
  UR_LOG_CUDA: "level:error;flush:error"
  UR_LOG_HIP: "level:error;flush:error"
  UR_LOG_LEVEL_ZERO: "level:error;flush:error"
  UR_LOG_NATIVE_CPU: "level:error;flush:error"
  UR_LOG_OPENCL: "level:error;flush:error"
  CURRENT_DIR: $(pwd)

jobs:
  adapter_build_hw:
    name: Build & CTS
    # run only on upstream; forks won't have the HW
    # if: github.repository == 'intel/llvm'
    strategy:
      fail-fast: false
      matrix:
        adapter: [
          {
            name: "${{inputs.adapter_name}}",
            other_name: "${{inputs.other_adapter_name}}",
            platform: "${{inputs.platform}}",
            static_Loader: "${{inputs.static_loader}}",
            static_adapter: "${{inputs.static_loader}}"
          }
        ]
        build_type: [Release]
        compiler: [{c: gcc, cxx: g++}]

    runs-on: ${{inputs.runner_name}}
    container:
      image: ${{ inputs.docker_image }}
      options: -u 1001 --device=/dev/dri -v /dev/dri/by-path:/dev/dri/by-path --privileged --cap-add SYS_ADMIN

    steps:
    # TODO:
    # - investigate if DUR_CONFORMANCE_AMD_ARCH could be removed
    # - find better way to handle platform param (e.g. "Intel(R) OpenCL" -> "opencl")
    # - switch to Ninja generator in CMake
    # - downloading DPC++ should be integrated somehow; most likely use nightly release.
    #
    - name: Checkout LLVM
      uses: actions/checkout@b4ffde65f46336ab88eb53be808477a3936bae11 # v4.1.1

    - name: Get information about platform
      if: ${{ always() }}
      run: ${{ env.CURRENT_DIR }}/unified-runtime/.github/scripts/get_system_info.sh

    - name: Set working directory as environment variable
      run: echo "CURRENT_DIR=$(pwd)" >> $GITHUB_ENV

    # Latest distros do not allow global pip installation
    - name: Install UR python dependencies in venv
      working-directory: ${{ env.CURRENT_DIR }}/unified-runtime
      run: |
        sudo apt update
        sudo apt install -y python3-venv
        python3 -m venv .venv
        . .venv/bin/activate
        echo "${PWD}/.venv/bin" >> $GITHUB_PATH
        pip install -r third_party/requirements.txt

    - name: Download DPC++
      run: |
        wget -O ${{ env.CURRENT_DIR }}/dpcpp_compiler.tar.gz https://github.com/intel/llvm/releases/download/nightly-2024-12-12/sycl_linux.tar.gz
        mkdir -p ${{ env.CURRENT_DIR }}/dpcpp_compiler
        tar -xvf ${{ env.CURRENT_DIR }}/dpcpp_compiler.tar.gz -C dpcpp_compiler

    - name: Install Intel Level Zero loader
      working-directory: ${{ env.CURRENT_DIR }}
      run: |
        curl -L https://github.com/oneapi-src/level-zero/archive/refs/heads/master.tar.gz -o level-zero.tar.gz
        tar -xzf level-zero.tar.gz
        cd level-zero-master
        
        mkdir build && cd build
        cmake .. -D CMAKE_BUILD_TYPE=Release -D CMAKE_INSTALL_PREFIX=usr/local/install
        cmake --build . --target package
        cmake --build . --target install

    - name: Install Intel Level Zero GPU
      run: |
        sudo apt-get update
        sudo apt-get install -y gnupg2 gpg-agent curl
        curl -fsSL https://repositories.intel.com/gpu/intel-graphics.key | sudo gpg --dearmor -o /usr/share/keyrings/intel-graphics.gpg
        echo "deb [arch=amd64 signed-by=/usr/share/keyrings/intel-graphics.gpg] https://repositories.intel.com/gpu/ubuntu jammy/lts/2350 unified" | sudo tee /etc/apt/sources.list.d/intel-gpu-jammy.list
        sudo apt update
        sudo apt install -y \
          intel-opencl-icd intel-level-zero-gpu \
          intel-media-va-driver-non-free libmfx1 libmfxgen1 libvpl2 \
          libegl-mesa0 libegl1-mesa libegl1-mesa-dev libgbm1 libgl1-mesa-dev libgl1-mesa-dri \
          libglapi-mesa libgles2-mesa-dev libglx-mesa0 libigdgmm12 libxatracker2 mesa-va-drivers \
          mesa-vdpau-drivers mesa-vulkan-drivers va-driver-all vainfo hwinfo clinfo

    - name: Add L0 to PATH
      run: |
        export PATH=/__w/llvm/llvm/level-zero-master/build/usr/local/install/bin:$PATH
        export LD_LIBRARY_PATH=/__w/llvm/llvm/level-zero-master/build/usr/local/install/lib:$LD_LIBRARY_PATH
        export CPATH=/__w/llvm/llvm/level-zero-master/build/usr/local/install/include:$CPATH
        export LIBRARY_PATH=/__w/llvm/llvm/level-zero-master/build/usr/local/install/lib:$LIBRARY_PATH

        echo 'export PATH=/__w/llvm/llvm/level-zero-master/build/usr/local/install/bin:$PATH' >> ~/.bashrc
        echo 'export LD_LIBRARY_PATH=/__w/llvm/llvm/level-zero-master/build/usr/local/install/lib:$LD_LIBRARY_PATH' >> ~/.bashrc
        echo 'export CPATH=/__w/llvm/llvm/level-zero-master/build/usr/local/install/include:$CPATH' >> ~/.bashrc
        echo 'export LIBRARY_PATH=/__w/llvm/llvm/level-zero-master/build/usr/local/install/lib:$LIBRARY_PATH' >> ~/.bashrc
        . ~/.bashrc
      shell: bash

    - name: Check PATH
      run: | 
        echo $PATH
        echo $LD_LIBRARY_PATH
        echo $CPATH
        echo $LIBRARY_PATH
        ls -l /__w/llvm/llvm/level-zero-master/build/usr/local/install/lib

    - name: Configure Unified Runtime project
      working-directory: ${{ env.CURRENT_DIR }}/unified-runtime
      # ">" is used to avoid adding "\" at the end of each line; this command is quite long
      run: >
        cmake
        -B${{ env.CURRENT_DIR }}/build
        -DCMAKE_C_COMPILER=${{matrix.compiler.c}}
        -DCMAKE_CXX_COMPILER=${{matrix.compiler.cxx}}
        -DCMAKE_BUILD_TYPE=${{matrix.build_type}}
        -DUR_ENABLE_TRACING=ON
        -DUR_DEVELOPER_MODE=ON
        -DUR_BUILD_TESTS=ON
        -DUR_BUILD_ADAPTER_${{matrix.adapter.name}}=ON
        -DUR_CONFORMANCE_TEST_LOADER=${{ matrix.adapter.other_name != '' && 'ON' || 'OFF' }}
        ${{ matrix.adapter.other_name != '' && format('-DUR_BUILD_ADAPTER_{0}=ON', matrix.adapter.other_name) || '' }}
        -DUR_STATIC_LOADER=${{matrix.adapter.static_Loader}}
        -DUR_STATIC_ADAPTER_${{matrix.adapter.name}}=${{matrix.adapter.static_adapter}}
        -DUR_DPCXX=${{ env.CURRENT_DIR }}/dpcpp_compiler/bin/clang++
        -DUR_SYCL_LIBRARY_DIR=${{ env.CURRENT_DIR }}/dpcpp_compiler/lib
        -DCMAKE_INSTALL_PREFIX=${{ env.CURRENT_DIR }}/install
        ${{ matrix.adapter.name == 'HIP' && '-DUR_CONFORMANCE_AMD_ARCH=gfx1030' || '' }}
        ${{ matrix.adapter.name == 'HIP' && '-DUR_HIP_PLATFORM=AMD' || '' }}

    - name: Build
      # This is so that device binaries can find the sycl runtime library
      run: cmake --build ${{ env.CURRENT_DIR }}/build -j $(nproc)

    - name: Install
      # This is to check that install command does not fail
      run: cmake --install ${{ env.CURRENT_DIR }}/build

    # - name: Run ldd on libur_* libraries
    #   run: |
    #     cd ${{ env.CURRENT_DIR }}
    #     pwd
    #     ls 
    #     cd build
    #     pwd
    #     ls
    #     ldd lib/libur_*

    # - name: Check if GPU is avaliable 
    #   run: |
    #     ls -la /dev/dri/
    #     id
    #     dpkg -l | grep level-zero
    #     ldconfig -p | grep libze
    #     ls -la /dev/dri/

    - name: Test adapter specific
      env:
        ZE_ENABLE_LOADER_DEBUG_TRACE: 1
        ZE_DEBUG: 1
      # run: ctest -C ${{matrix.build_type}} --test-dir ${{ env.CURRENT_DIR }}/build --output-on-failure -L "adapter-specific" -E "memcheck" --timeout 600 -VV
      run: ctest -C ${{ matrix.build_type }} --test-dir ${{ env.CURRENT_DIR }}/build --output-on-failure -L "adapter-specific" -R "test-adapter-level_zero$" --timeout 600 -VV
      # Don't run adapter specific tests when building multiple adapters
      if: ${{ matrix.adapter.other_name == '' }}

    - name: Test adapters
      env:
        ZE_ENABLE_LOADER_DEBUG_TRACE: 1
        ZE_DEBUG: 1
      run: env UR_CTS_ADAPTER_PLATFORM="${{matrix.adapter.platform}}" ctest -C ${{matrix.build_type}} --test-dir ${{ env.CURRENT_DIR }}/build --output-on-failure -L "conformance" --timeout 600 -VV

    - name: Get information about platform
      if: ${{ always() }}
      run: ${{ env.CURRENT_DIR }}/unified-runtime/.github/scripts/get_system_info.sh
